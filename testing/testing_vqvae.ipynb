{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "\n",
    "from datasets import OrganoidDataset\n",
    "import torch\n",
    "\n",
    "data = OrganoidDataset(data_dir='/data/PycharmProjects/cytof_benchmark/data/organoids')\n",
    "\n",
    "X_train, y_train = data.train\n",
    "X_val, y_val = data.val\n",
    "\n",
    "X_train_batches = torch.split(torch.Tensor(X_train).to('cuda'), split_size_or_sections=32*1024)\n",
    "if X_train_batches[-1].shape[0] < 16*1024:\n",
    "    X_train_batches = X_train_batches[:-1]\n",
    "\n",
    "X_val_batches = torch.split(torch.Tensor(X_val).to('cuda'), split_size_or_sections=32*1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from models.vqvae import *\n",
    "from configs.vqvae import get_config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32768, 41])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X_train_batches[0]\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "batch_size: 32768\ndataset: Organoid\nembed_dim1: 2\nembed_dim2: 4\nembed_dim3: 5\nembed_entries1: 16\nembed_entries2: 16\nembed_entries3: 32\nhidden_features: 64\nin_features: 41\nkld_scale: 0.0005\nmodel: VQVAE\nn_layers: 3\noutput_dir: ./logs/VQVAE/\nseed: 12345\nstraight_through: false\ntemperature: 1"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = get_config()\n",
    "config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_parameter_count(net: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 69559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 199, Loss: 0.3949335068464279, MSE: 0.39089666306972504, KLD: 8.073681950569153: 100%|██████████| 200/200 [01:15<00:00,  2.66it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "config = get_config()\n",
    "model = VQVAE(config=config).to('cuda')\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f\"Model parameters: {get_parameter_count(model)}\")\n",
    "\n",
    "epochs = 200\n",
    "optimizer = optim.AdamW(model.parameters(),\n",
    "                       lr=0.005,\n",
    "                       )\n",
    "\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "pbar = tqdm(range(epochs))\n",
    "\n",
    "for epoch in pbar:\n",
    "    for X_batch in X_train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(X_batch)\n",
    "        loss = model.loss_function(*outputs)\n",
    "        loss['loss'].backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses = list()\n",
    "        for X_batch in X_val_batches:\n",
    "            loss_dict=dict()\n",
    "            outputs = model.forward(X_batch)\n",
    "            loss_val = model.loss_function(*outputs)\n",
    "            loss_dict['loss'] = loss_val['loss'].to('cpu').numpy().item()\n",
    "            loss_dict['MSE'] = loss_val['MSE'].to('cpu').numpy().item()\n",
    "            loss_dict['KLD'] = loss_val['KLD'].to('cpu').numpy().item()\n",
    "            losses.append(loss_dict)\n",
    "\n",
    "        loss = np.mean([loss['loss'] for loss in losses])\n",
    "        rec_loss = np.mean([loss['MSE'] for loss in losses])\n",
    "        mmd_loss = np.mean([loss['KLD'] for loss in losses])\n",
    "        pbar.set_description(f\"Epoch: {epoch}, \"\n",
    "                             f\"Loss: {loss}, \"\n",
    "                             f\"MSE: {rec_loss}, \"\n",
    "                             f\"KLD: {mmd_loss}\", refresh=True)\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
