{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "class HelperModule(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.build(*args, **kwargs)\n",
    "\n",
    "    def build(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "def get_parameter_count(net: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "def get_device(cpu):\n",
    "    if cpu or not torch.cuda.is_available(): return torch.device('cpu')\n",
    "    return torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class ReZero(HelperModule):\n",
    "    def build(self, in_features: int):\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.BatchNorm1d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return self.layers(x) * self.alpha + x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class ResidualStack(HelperModule):\n",
    "    def build(self, in_features: int, nb_layers: int):\n",
    "        self.stack = nn.Sequential(*[ReZero(in_features) for _ in range(nb_layers)])\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return self.stack(x)\n",
    "\n",
    "class Encoder(HelperModule):\n",
    "    def build(self,\n",
    "            in_features: int, hidden_features: int, nb_res_layers: int\n",
    "        ):\n",
    "        layers = []\n",
    "\n",
    "        if in_features!=hidden_features:\n",
    "            layers.append(nn.Linear(in_features, hidden_features))\n",
    "\n",
    "        layers.append(ResidualStack(hidden_features, nb_res_layers))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return self.layers(x)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "class Decoder(HelperModule):\n",
    "    def build(self,\n",
    "            embed_dim: int, hidden_features: int, out_dim:int, nb_res_layers: int\n",
    "        ):\n",
    "        layers = []\n",
    "        if embed_dim!=hidden_features:\n",
    "            layers.append(nn.Linear(embed_dim, hidden_features))\n",
    "\n",
    "        layers.append(ResidualStack(hidden_features, nb_res_layers))\n",
    "\n",
    "        if out_dim!=hidden_features:\n",
    "            layers.append(nn.Linear(hidden_features, out_dim))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return self.layers(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "class CodeLayer(HelperModule):\n",
    "    def build(self, in_features: int, embed_dim: int, nb_entries: int):\n",
    "        self.linear_in = nn.Linear(in_features, embed_dim)\n",
    "\n",
    "        self.dim = embed_dim\n",
    "        self.n_embed = nb_entries\n",
    "        self.decay = 0.99\n",
    "        self.eps = 1e-5\n",
    "\n",
    "        embed = torch.randn(embed_dim, nb_entries, dtype=torch.float32)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(nb_entries, dtype=torch.float32))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def forward(self, x: torch.FloatTensor) -> Tuple[torch.FloatTensor, float, torch.LongTensor]:\n",
    "        x = self.linear_in(x.float())\n",
    "\n",
    "        dist = (\n",
    "            x.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * x @ self.embed\n",
    "            + self.embed.pow(2).sum(0, keepdim=True)\n",
    "        )\n",
    "        _, embed_ind = (-dist).max(1)\n",
    "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
    "\n",
    "        quantize = self.embed_code(embed_ind)\n",
    "\n",
    "        if self.training:\n",
    "            embed_onehot_sum = embed_onehot.sum(0)\n",
    "            embed_sum = x.transpose(0, 1) @ embed_onehot\n",
    "\n",
    "            # TODO: Replace this? Or can we simply comment out?\n",
    "            # dist_fn.all_reduce(embed_onehot_sum)\n",
    "            # dist_fn.all_reduce(embed_sum)\n",
    "\n",
    "            self.cluster_size.data.mul_(self.decay).add_(\n",
    "                embed_onehot_sum, alpha=1 - self.decay\n",
    "            )\n",
    "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = (\n",
    "                (self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n\n",
    "            )\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        diff = (quantize.detach() - x).pow(2).mean()\n",
    "        quantize = x + (quantize - x).detach()\n",
    "\n",
    "        return quantize, diff, embed_ind\n",
    "\n",
    "    def embed_code(self, embed_id: torch.LongTensor) -> torch.FloatTensor:\n",
    "        return F.embedding(embed_id, self.embed.transpose(0, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class VQVAE(HelperModule):\n",
    "    def build(self,\n",
    "            in_channels: int                = 3,\n",
    "            hidden_channels: int            = 128,\n",
    "            res_channels: int               = 32,\n",
    "            nb_res_layers: int              = 2,\n",
    "            nb_levels: int                  = 3,\n",
    "            embed_dim: int                  = 64,\n",
    "            nb_entries: int                 = 512,\n",
    "            scaling_rates: list[int]        = [8, 4, 2]\n",
    "        ):\n",
    "        self.nb_levels = nb_levels\n",
    "        assert len(scaling_rates) == nb_levels, \"Number of scaling rates not equal to number of levels!\"\n",
    "\n",
    "        self.encoders = nn.ModuleList([Encoder(in_channels, hidden_channels, res_channels, nb_res_layers, scaling_rates[0])])\n",
    "        for i, sr in enumerate(scaling_rates[1:]):\n",
    "            self.encoders.append(Encoder(hidden_channels, hidden_channels, res_channels, nb_res_layers, sr))\n",
    "\n",
    "        self.codebooks = nn.ModuleList()\n",
    "        for i in range(nb_levels - 1):\n",
    "            self.codebooks.append(CodeLayer(hidden_channels+embed_dim, embed_dim, nb_entries))\n",
    "        self.codebooks.append(CodeLayer(hidden_channels, embed_dim, nb_entries))\n",
    "\n",
    "        self.decoders = nn.ModuleList([Decoder(embed_dim*nb_levels, hidden_channels, in_channels, res_channels, nb_res_layers, scaling_rates[0])])\n",
    "        for i, sr in enumerate(scaling_rates[1:]):\n",
    "            self.decoders.append(Decoder(embed_dim*(nb_levels-1-i), hidden_channels, embed_dim, res_channels, nb_res_layers, sr))\n",
    "\n",
    "        self.upscalers = nn.ModuleList()\n",
    "        for i in range(nb_levels - 1):\n",
    "            self.upscalers.append(Upscaler(embed_dim, scaling_rates[1:len(scaling_rates) - i][::-1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_outputs = []\n",
    "        code_outputs = []\n",
    "        decoder_outputs = []\n",
    "        upscale_counts = []\n",
    "        id_outputs = []\n",
    "        diffs = []\n",
    "\n",
    "        for enc in self.encoders:\n",
    "            if len(encoder_outputs):\n",
    "                encoder_outputs.append(enc(encoder_outputs[-1]))\n",
    "            else:\n",
    "                encoder_outputs.append(enc(x))\n",
    "\n",
    "        for l in range(self.nb_levels-1, -1, -1):\n",
    "            codebook, decoder = self.codebooks[l], self.decoders[l]\n",
    "\n",
    "            if len(decoder_outputs): # if we have previous levels to condition on\n",
    "                code_q, code_d, emb_id = codebook(torch.cat([encoder_outputs[l], decoder_outputs[-1]], axis=1))\n",
    "            else:\n",
    "                code_q, code_d, emb_id = codebook(encoder_outputs[l])\n",
    "            diffs.append(code_d)\n",
    "            id_outputs.append(emb_id)\n",
    "\n",
    "            code_outputs = [self.upscalers[i](c, upscale_counts[i]) for i, c in enumerate(code_outputs)]\n",
    "            upscale_counts = [u+1 for u in upscale_counts]\n",
    "            decoder_outputs.append(decoder(torch.cat([code_q, *code_outputs], axis=1)))\n",
    "\n",
    "            code_outputs.append(code_q)\n",
    "            upscale_counts.append(0)\n",
    "\n",
    "        return decoder_outputs[-1], diffs, encoder_outputs, decoder_outputs, id_outputs\n",
    "\n",
    "    def decode_codes(self, *cs):\n",
    "        decoder_outputs = []\n",
    "        code_outputs = []\n",
    "        upscale_counts = []\n",
    "\n",
    "        for l in range(self.nb_levels - 1, -1, -1):\n",
    "            codebook, decoder = self.codebooks[l], self.decoders[l]\n",
    "            code_q = codebook.embed_code(cs[l]).permute(0, 3, 1, 2)\n",
    "            code_outputs = [self.upscalers[i](c, upscale_counts[i]) for i, c in enumerate(code_outputs)]\n",
    "            upscale_counts = [u+1 for u in upscale_counts]\n",
    "            decoder_outputs.append(decoder(torch.cat([code_q, *code_outputs], axis=1)))\n",
    "\n",
    "            code_outputs.append(code_q)\n",
    "            upscale_counts.append(0)\n",
    "\n",
    "        return decoder_outputs[-1]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "\n",
    "from datasets import OrganoidDataset\n",
    "\n",
    "data = OrganoidDataset(data_dir='/home/egor/PycharmProjects/deep_dr/data/organoids')\n",
    "\n",
    "X_train, y_train = data.train\n",
    "X_val, y_val = data.val\n",
    "\n",
    "X_train_batches = torch.split(torch.Tensor(X_train), split_size_or_sections=1024)\n",
    "X_val_batches = torch.split(torch.Tensor(X_val), split_size_or_sections=1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 41])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_batches[0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "enc = Encoder(in_features = 41, hidden_features = 32, nb_res_layers=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 41])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X_train_batches[0]\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 32])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = enc.forward(x)\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "code = CodeLayer(in_features = 32, embed_dim=16, nb_entries=256)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.3911,  0.6807, -0.2599,  ...,  0.6381,  0.6656,  0.3845],\n        [-0.1961,  0.8031, -0.0063,  ...,  0.8630,  0.1975,  0.3984],\n        [-0.1961,  0.8031, -0.0063,  ...,  0.8630,  0.1975,  0.3984],\n        ...,\n        [-0.1961,  0.8031, -0.0063,  ...,  0.8630,  0.1975,  0.3984],\n        [-0.3911,  0.6807, -0.2599,  ...,  0.6381,  0.6656,  0.3845],\n        [-0.1961,  0.8031, -0.0063,  ...,  0.8630,  0.1975,  0.3984]],\n       grad_fn=<AddBackward0>)"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code(x)[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "dec = Decoder(embed_dim=16, hidden_features=32, out_dim=41, nb_res_layers = 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 41])"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(code(x)[0]).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "embed = torch.randn(32, 256, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 256])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "flatten = x.permute(1,0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 1024])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 1])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.pow(2).sum(1, keepdim=True).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 256])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x @ embed).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "dist = (\n",
    "            x.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * x @ embed\n",
    "            + embed.pow(2).sum(0, keepdim=True)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 256])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "_, embed_ind = (-dist).max(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 256])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot = F.one_hot(embed_ind, 256).type(flatten.dtype)\n",
    "embed_onehot.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "quantize = F.embedding(embed_ind, embed.transpose(0,1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 32])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "embed_onehot_sum = embed_onehot.sum(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([256])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot_sum.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "embed_sum = x.transpose(0, 1) @ embed_onehot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 256])"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_sum.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "\n",
    "diff = (quantize.detach() - x).pow(2).mean()\n",
    "quantize = x + (quantize - x).detach()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.5068, grad_fn=<MeanBackward0>)"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([245, 245, 191,  ..., 245, 204, 245])"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
