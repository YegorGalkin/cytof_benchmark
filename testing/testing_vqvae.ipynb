{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "\n",
    "from datasets import OrganoidDataset\n",
    "import torch\n",
    "\n",
    "data = OrganoidDataset(data_dir='/home/egor/PycharmProjects/deep_dr/data/organoids')\n",
    "\n",
    "X_train, y_train = data.train\n",
    "X_val, y_val = data.val\n",
    "\n",
    "X_train_batches = torch.split(torch.Tensor(X_train), split_size_or_sections=32*1024)\n",
    "X_val_batches = torch.split(torch.Tensor(X_val), split_size_or_sections=32*1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from models.vqvae import *\n",
    "from configs.vqvae import get_config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32768, 41])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X_train_batches[0]\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "batch_size: 32768\ndataset: Organoid\nembed_dim1: 2\nembed_dim2: 4\nembed_dim3: 5\nembed_entries1: 16\nembed_entries2: 16\nembed_entries3: 32\nhidden_features: 32\nin_features: 41\nkld_scale: 0.0005\nmodel: VQVAE\nn_layers: 3\noutput_dir: ./logs/VQVAE/\nseed: 12345\nstraight_through: false\ntemperature: 1"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = get_config()\n",
    "config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_parameter_count(net: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 23575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Loss: 0.4861161895096302, MSE: 0.48189911618828773, KLD: 8.434143900871277:  17%|█▋        | 34/200 [02:56<14:23,  5.20s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [7], line 30\u001B[0m\n\u001B[1;32m     28\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mforward(X_batch)\n\u001B[1;32m     29\u001B[0m     loss \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mloss_function(\u001B[38;5;241m*\u001B[39moutputs)\n\u001B[0;32m---> 30\u001B[0m     \u001B[43mloss\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mloss\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     32\u001B[0m scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/PycharmProjects/deep_dr/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/deep_dr/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "config = get_config()\n",
    "model = VQVAE(config=config)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f\"Model parameters: {get_parameter_count(model)}\")\n",
    "\n",
    "epochs = 200\n",
    "optimizer = optim.AdamW(model.parameters(),\n",
    "                       lr=0.01,\n",
    "                       )\n",
    "\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "pbar = tqdm(range(epochs))\n",
    "\n",
    "for epoch in pbar:\n",
    "    for X_batch in X_train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(X_batch)\n",
    "        loss = model.loss_function(*outputs)\n",
    "        loss['loss'].backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses = list()\n",
    "        for X_batch in X_val_batches:\n",
    "            loss_dict=dict()\n",
    "            outputs = model.forward(X_batch)\n",
    "            loss_val = model.loss_function(*outputs)\n",
    "            loss_dict['loss'] = loss_val['loss'].to('cpu').numpy().item()\n",
    "            loss_dict['MSE'] = loss_val['MSE'].to('cpu').numpy().item()\n",
    "            loss_dict['KLD'] = loss_val['KLD'].to('cpu').numpy().item()\n",
    "            losses.append(loss_dict)\n",
    "\n",
    "        loss = np.mean([loss['loss'] for loss in losses])\n",
    "        rec_loss = np.mean([loss['MSE'] for loss in losses])\n",
    "        mmd_loss = np.mean([loss['KLD'] for loss in losses])\n",
    "        pbar.set_description(f\"Epoch: {epoch}, \"\n",
    "                             f\"Loss: {loss}, \"\n",
    "                             f\"MSE: {rec_loss}, \"\n",
    "                             f\"KLD: {mmd_loss}\", refresh=True)\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
