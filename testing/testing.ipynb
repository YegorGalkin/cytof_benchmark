{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egor/PycharmProjects/deep_dr/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import OrganoidDataset\n",
    "data = OrganoidDataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([820733, 41]) torch.Size([820733, 2])\n",
      "torch.Size([234495, 41]) torch.Size([234495, 2])\n",
      "torch.Size([117248, 41]) torch.Size([117248, 2])\n"
     ]
    }
   ],
   "source": [
    "print(data.train[0].shape,data.train[1].shape)\n",
    "print(data.val[0].shape,data.val[1].shape)\n",
    "print(data.test[0].shape,data.test[1].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from models import VanillaVAE\n",
    "model = VanillaVAE(in_features=41, hidden_dims = [41,41,41], latent_dim= 2).to('cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X_train,y_train = data.train\n",
    "X_val,y_val = data.val"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.005,\n",
    "                       weight_decay=0.0,\n",
    "                       )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 140, Loss: 0.49964088201522827, Reconstruction loss: 0.4841030538082123, KL loss: -6.215127468109131:  15%|█▍        | 146/1000 [00:16<01:36,  8.88it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [7], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m pbar:\n\u001B[1;32m      6\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m----> 7\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     loss \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mloss_function(\u001B[38;5;241m*\u001B[39moutputs, M_N\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0025\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      9\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/PycharmProjects/deep_dr/models/vanilla_vae.py:110\u001B[0m, in \u001B[0;36mVanillaVAE.forward\u001B[0;34m(self, input, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m mu, log_var \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m    109\u001B[0m z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreparameterize(mu, log_var)\n\u001B[0;32m--> 110\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28minput\u001B[39m, mu, log_var]\n",
      "File \u001B[0;32m~/PycharmProjects/deep_dr/models/vanilla_vae.py:92\u001B[0m, in \u001B[0;36mVanillaVAE.decode\u001B[0;34m(self, z)\u001B[0m\n\u001B[1;32m     90\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder_input(z)\n\u001B[1;32m     91\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(result)\n\u001B[0;32m---> 92\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinal_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/PycharmProjects/deep_dr/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/deep_dr/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/deep_dr/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/deep_dr/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm(range(1000))\n",
    "\n",
    "for epoch in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model.forward(X_train)\n",
    "    loss = model.loss_function(*outputs, M_N=0.0025)['loss']\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.forward(X_val)\n",
    "            loss = model.loss_function(*outputs, M_N=0.0025)\n",
    "\n",
    "            pbar.set_description(f\"Epoch: {epoch}, \"\n",
    "                             f\"Loss: {loss['loss']}, \"\n",
    "                             f\"Reconstruction loss: {loss['Reconstruction_Loss']}, \"\n",
    "                             f\"KL loss: {loss['KLD']}\", refresh=True)\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'logs/VanillaVAE/latent=2_epoch=10000_layers=41,41,41.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35452994 0.18482029 0.05850656 0.04639866 0.03060609 0.02742573\n",
      " 0.02369693 0.02089108 0.01877983 0.01733262]\n",
      "[0.35452994 0.53935023 0.59785679 0.64425544 0.67486153 0.70228726\n",
      " 0.72598418 0.74687526 0.76565509 0.78298772]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "pca = PCA(n_components=10)\n",
    "train_data = X_train.cpu()\n",
    "pca.fit(train_data)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components: 0, MSE:1.3534091070398027\n",
      "Components: 1, MSE:0.8742205681688913\n",
      "Components: 2, MSE:0.6241996179389656\n",
      "Components: 3, MSE:0.544869697229827\n",
      "Components: 4, MSE:0.48213737385965566\n",
      "Components: 5, MSE:0.44071113664262956\n",
      "Components: 6, MSE:0.4035927459006117\n",
      "Components: 7, MSE:0.3716002847620463\n",
      "Components: 8, MSE:0.3433178042269015\n",
      "Components: 9, MSE:0.31782799266804174\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "val_data = X_val.cpu()\n",
    "for i in range(10):\n",
    "    reduced_dim = np.pad(pca.transform(val_data)[:,:i],((0,0),(0,10-i)))\n",
    "    loss = F.mse_loss(val_data, torch.from_numpy(pca.inverse_transform(reduced_dim)))\n",
    "    print(f\"Components: {i}, MSE:{loss}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    latent_val = model.reparameterize(*model.encode(X_val)).to('cpu')\n",
    "    outputs = model.forward(X_val)\n",
    "    loss = model.loss_function(*outputs, M_N=0.0025)\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import CellType\n",
    "reduced_dims = pd.DataFrame(y_val.cpu()).rename({0:\"Cell type\",1:\"Day\"},axis=1)\n",
    "reduced_dims['Cell type'].replace({i.value: i.name for i in CellType}, inplace=True)\n",
    "\n",
    "reduced_dims[\"PC1\"]= pca.transform(X_val.cpu())[:,0]\n",
    "reduced_dims[\"PC2\"]= pca.transform(X_val.cpu())[:,1]\n",
    "reduced_dims[\"VAE1\"] = latent_val[:,0]\n",
    "reduced_dims[\"VAE2\"] = latent_val[:,1]\n",
    "reduced_dims"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lmplot( x=\"PC1\", y=\"PC2\",\n",
    "  data=reduced_dims.head(10000),\n",
    "  fit_reg=False,\n",
    "  hue='Cell type', # color by cluster\n",
    "  legend=True,\n",
    "  scatter_kws={\"s\": 5})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lmplot( x=\"VAE1\", y=\"VAE2\",\n",
    "  data=reduced_dims.head(10000),\n",
    "  fit_reg=False,\n",
    "  hue='Cell type', # color by cluster\n",
    "  legend=True,\n",
    "  scatter_kws={\"s\": 5})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lmplot( x=\"PC1\", y=\"PC2\",\n",
    "  data=reduced_dims.head(10000),\n",
    "  fit_reg=False,\n",
    "  hue='Day', # color by cluster\n",
    "  legend=True,\n",
    "  scatter_kws={\"s\": 5})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lmplot( x=\"VAE1\", y=\"VAE2\",\n",
    "  data=reduced_dims.head(10000),\n",
    "  fit_reg=False,\n",
    "  hue='Day', # color by cluster\n",
    "  legend=True,\n",
    "  scatter_kws={\"s\": 5})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import OrganoidDataset\n",
    "\n",
    "in_features=41\n",
    "latent_dim=2\n",
    "hidden_dims = [41,32,16]\n",
    "kld_weight=0.0025\n",
    "output_dir = './logs/VanillaVAE/'\n",
    "lr = 0.005\n",
    "weight_decay = 0.0\n",
    "epochs=1000\n",
    "save_loss_every_n_epochs=10\n",
    "dataset=OrganoidDataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import VanillaVAE\n",
    "print(f\"CUDA available:{torch.cuda.is_available()}\")\n",
    "data = dataset()\n",
    "model = VanillaVAE(in_features=in_features, hidden_dims=hidden_dims, latent_dim=latent_dim).to('cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "for _ in range(1):\n",
    "    X_train, y_train = data.train\n",
    "    X_val, y_val = data.val\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=lr,\n",
    "                           weight_decay=weight_decay,\n",
    "                           )\n",
    "\n",
    "    loss_list = list()\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(X_train)\n",
    "        loss = model.loss_function(*outputs, M_N=kld_weight)\n",
    "        loss['loss'].backward()\n",
    "        optimizer.step()\n",
    "        if epoch % save_loss_every_n_epochs == 0 or epoch == epochs:\n",
    "            with torch.no_grad():\n",
    "                loss_dict = dict()\n",
    "                loss_dict['epoch'] = epoch\n",
    "                loss_dict['train_loss'] = loss['loss'].to('cpu').numpy().item()\n",
    "                loss_dict['train_rec_loss'] = loss['Reconstruction_Loss'].to('cpu').numpy().item()\n",
    "                loss_dict['train_kld'] = loss['KLD'].to('cpu').numpy().item()\n",
    "\n",
    "                outputs_val = model.forward(X_val)\n",
    "                loss_val = model.loss_function(*outputs_val, M_N=kld_weight)\n",
    "\n",
    "                loss_dict['val_loss'] = loss_val['loss'].to('cpu').numpy().item()\n",
    "                loss_dict['val_rec_loss'] = loss_val['Reconstruction_Loss'].to('cpu').numpy().item()\n",
    "                loss_dict['val_kld'] = loss_val['KLD'].to('cpu').numpy().item()\n",
    "\n",
    "                loss_list.append(loss_dict)\n",
    "\n",
    "    print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(loss_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    latent_val = model.reparameterize(*model.encode(X_val)).to('cpu')\n",
    "    reconstructed_val = model.generate(X_val).to('cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from experiment import run_vae"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_vae(in_features=41,epochs=50000,hidden_dims=[41,32,16,8,4],save_loss_every_n_epochs=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'a'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m.c\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43ma.b\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:992\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:992\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1004\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'a'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.import_module('.c', 'a.b')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'(1, 2, 3, 4, 5)'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(tuple([1,2,3,4,5]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['\"(128, 128)\"',\n '\"(128, 128, 128)\"',\n '\"(128, 128, 128, 128)\"',\n '\"(128, 128, 128, 128, 128)\"',\n '\"(128, 128, 128, 128, 128, 128)\"',\n '\"(128, 128, 128, 128, 128, 128, 128)\"',\n '\"(128, 128, 128, 128, 128, 128, 128, 128)\"']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from models import BetaVAE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/data/PycharmProjects/cytof_benchmark/logs/BetaVAE/exp_5/run_5/config.pkl', 'rb') as file:\n",
    "    config_best = pickle.load(file)\n",
    "model_best = BetaVAE(config_best).to(config_best.device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def test(model,val_dataloader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        mse,kld,loss = list(),list(),list()\n",
    "        for X_batch in val_dataloader:\n",
    "            loss_dict = dict()\n",
    "            outputs = model.forward(X_batch)\n",
    "            losses = model.loss_function(*outputs)\n",
    "            for key in losses.keys():\n",
    "                loss_dict[key] = losses[key].item() * X_batch.shape[0]\n",
    "            mse.append(loss_dict['MSE'])\n",
    "            kld.append(loss_dict['KLD'])\n",
    "            loss.append(loss_dict['loss'])\n",
    "    data_len = sum(len(batch) for batch in val_dataloader)\n",
    "    return sum(mse)/data_len,sum(kld)/data_len, sum(loss)/data_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import OrganoidDataset\n",
    "\n",
    "data = OrganoidDataset()\n",
    "\n",
    "X_train, y_train = data.train\n",
    "X_val, y_val = data.val\n",
    "X_train_batches = torch.split(X_train, split_size_or_sections=1024)\n",
    "X_val_batches = torch.split(X_val, split_size_or_sections=1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/data/PycharmProjects/cytof_benchmark/logs/BetaVAE/exp_5/run_5/model.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best.load_state_dict(checkpoint)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.2957925220464002, -10.51027182543638, 0.32206820037098627)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_best,X_val_batches)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
