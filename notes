- Run a separate test for batch size of 32K for old runner and the new runner, the runtime is close to 15 min in both cases (<5% difference)
- New 16M cell datasets take a lot of GPU memory, so i changed datasets pipeline from GPU in-memory to streaming using
ray air datasets from NVME SSD. I've set batch size to maximum, and reduced population to 16, however there is 50% drop in training speed.
Next experiment is running the same PBT with same population and batch size optimization. Loss is 0.2 higher, which is a lot
- With batch included in scheduling, the loss seemed to improve, however speed is ~ 3 times slower. loss=0.343 after 3 hours, max iteration is 1296
- Trying to turn off prefetch, added dataset partitioning. loss is 0.349 after 1.5 hours, with 680 max iteration
- Trying to create dataset manager for each runner with prefetch. Didn't work
- Put whole dataset into ray memory storage, no more disk cache. Feeling lucky, removed blocking, 32 parallel, pinned memory
- Turned out to be slower. Moved datasets to cuda memory. Looks faster?
- Looks like 35% slower then previous fastest configuration. Running for full 8 hours. If results are only 30% slower then keep.
- Approximately 3 times slower. It actually still serializes in memory and deserializes in workers.
- 2 times slower with shared numpy array and ray dataset instances
- Trying tensordataset instances with multiprocess loading via pytorch DataLoader still 50% slower
- 4 concurrent workers, load whole dataset into memory
- The previous best result was around 3000 iterations x 32 workers per 8 hours.
I think shared TensorDataset with multiprocess dataloader gave around 4000 iterations, need to retry the experiment