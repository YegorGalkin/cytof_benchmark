{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 14:54:52,163\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "2023-02-17 14:54:54,547\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n"
     ]
    }
   ],
   "source": [
    "from ray.tune import ExperimentAnalysis\n",
    "ray.init()\n",
    "analysis = ExperimentAnalysis(\"/home/egor/Desktop/ray_tune/pbt_bench/BetaVAE/OrganoidDataset/beta_vae_training\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "To fetch the `best_result`, pass a `metric` and `mode` parameter to `tune.run()`. Alternatively, use `get_best_trial(metric, mode).last_result` to set the metric and mode explicitly and fetch the last result.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43manalysis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_result_df\u001B[49m\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/tune/analysis/experiment_analysis.py:340\u001B[0m, in \u001B[0;36mExperimentAnalysis.best_result_df\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m pd:\n\u001B[1;32m    335\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    336\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`best_result_df` requires pandas. Install with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    337\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`pip install pandas`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    338\u001B[0m     )\n\u001B[0;32m--> 340\u001B[0m best_result \u001B[38;5;241m=\u001B[39m flatten_dict(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_result\u001B[49m, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_delimiter())\n\u001B[1;32m    341\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39mfrom_records([best_result], index\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrial_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/tune/analysis/experiment_analysis.py:313\u001B[0m, in \u001B[0;36mExperimentAnalysis.best_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    304\u001B[0m \u001B[38;5;124;03m\"\"\"Get the last result of the best trial of the experiment\u001B[39;00m\n\u001B[1;32m    305\u001B[0m \n\u001B[1;32m    306\u001B[0m \u001B[38;5;124;03mThe best trial is determined by comparing the last trial results\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;124;03m`get_best_trial(metric, mode, scope).last_result` instead.\u001B[39;00m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_metric \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_mode:\n\u001B[0;32m--> 313\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    314\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo fetch the `best_result`, pass a `metric` and `mode` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter to `tune.run()`. Alternatively, use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    316\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`get_best_trial(metric, mode).last_result` to set \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    317\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe metric and mode explicitly and fetch the last result.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    318\u001B[0m     )\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_trial\u001B[38;5;241m.\u001B[39mlast_result\n",
      "\u001B[0;31mValueError\u001B[0m: To fetch the `best_result`, pass a `metric` and `mode` parameter to `tune.run()`. Alternatively, use `get_best_trial(metric, mode).last_result` to set the metric and mode explicitly and fetch the last result."
     ]
    }
   ],
   "source": [
    "analysis.best_result_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "best_result = analysis.get_best_trial(metric=\"loss\", mode=\"min\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.concat(analysis.trial_dataframes).to_csv('/data/PycharmProjects/cytof_benchmark/results/benchmark_loss_dynamics.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "['/home/egor/Desktop/ray_tune/pbt_bench/HyperSphericalVAE/ChallengeDataset/hs_vae_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/HyperSphericalVAE/CafDataset/hs_vae_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/HyperSphericalVAE/OrganoidDataset/hs_vae_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/DBetaVAE/ChallengeDataset/dbeta_vae_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/DBetaVAE/CafDataset/dbeta_vae_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/DBetaVAE/OrganoidDataset/dbeta_vae_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/BetaVAE/ChallengeDataset/beta_vae_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/BetaVAE/CafDataset/beta_vae_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/BetaVAE/OrganoidDataset/beta_vae_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/WAE_MMD/ChallengeDataset/wae_mmd_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/WAE_MMD/CafDataset/wae_mmd_training',\n '/home/egor/Desktop/ray_tune/pbt_bench/WAE_MMD/OrganoidDataset/wae_mmd_training']"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "bench_dir = \"/home/egor/Desktop/ray_tune/pbt_bench/\"\n",
    "dir_list = glob.glob(bench_dir + \"*/*/*_training\")\n",
    "dir_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 15:42:15,072\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,166\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,232\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,312\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,410\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,482\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,563\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,661\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,730\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,773\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,816\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n",
      "2023-02-17 15:42:15,855\tINFO experiment_analysis.py:795 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n"
     ]
    }
   ],
   "source": [
    "experiments = [ExperimentAnalysis(d) for d in dir_list]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "trial_dfs = [pd.concat(exp.trial_dataframes) for exp in experiments]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "'ChallengeDataset'"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_list[0].split('/')[-2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "for i,trial_df in enumerate(trial_dfs):\n",
    "    trial_df['dataset']=dir_list[i].split('/')[-2]\n",
    "    trial_df['model']=dir_list[i].split('/')[-3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "pd.concat(trial_dfs).to_csv('/data/PycharmProjects/cytof_benchmark/results/benchmark_loss_dynamics.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
