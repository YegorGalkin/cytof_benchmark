{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 16:12:55.579337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-10 16:12:55.699479: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-10 16:12:56.183287: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-10 16:12:56.183336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-10 16:12:56.183342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-10 16:12:56.931474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-10 16:12:56.948018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-10 16:12:56.948195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 16:13:03.245013: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds, ds_info = tfds.load('organoids', split='train',batch_size=64, with_info=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.         3.8617337  4.571121   ... 0.         4.8368387  7.9113965 ]\n",
      " [0.         3.0613792  5.5842743  ... 0.         5.43245    7.023813  ]\n",
      " [0.         2.2419517  3.158241   ... 0.         2.271955   4.4754205 ]\n",
      " ...\n",
      " [0.17605919 4.2487483  2.9109216  ... 5.0950274  4.776161   5.8817267 ]\n",
      " [0.29639632 2.8709605  2.5053656  ... 0.         2.811519   5.378569  ]\n",
      " [0.24135396 4.15895    3.709844   ... 0.         3.0586607  6.2270145 ]], shape=(64, 41), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for batch in ds.take(1):\n",
    "    print(batch['expression'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['Enterocyte', 'Enteroendocrine', 'Goblet', 'Paneth', 'Stem', 'Tuft']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_info.features['cell_type'].names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Sequence, Callable, Any\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import random\n",
    "\n",
    "ModuleDef = Any\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    latents: int\n",
    "    hidden_dims: Sequence[int]\n",
    "    norm:ModuleDef\n",
    "    act: Callable\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for i, hdim in enumerate(self.hidden_dims):\n",
    "            x = nn.Dense(hdim, name=f'fc_enc_{i}_{hdim}')(x)\n",
    "            x = self.act()(x)\n",
    "            x = self.norm()(x)\n",
    "\n",
    "        mean_x = nn.Dense(self.latents, name='latent_mean')(x)\n",
    "        logvar_x = nn.Dense(self.latents, name='latent_logvar')(x)\n",
    "        return mean_x, logvar_x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    hidden_dims: Sequence[int]\n",
    "    input_features: int\n",
    "    norm:ModuleDef\n",
    "    act: Callable\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, z):\n",
    "        for i, hdim in enumerate(reversed(self.hidden_dims)):\n",
    "            z = nn.Dense(hdim, name=f'fc_dec_{i}_{hdim}')(z)\n",
    "            z = self.act()(z)\n",
    "            z = self.norm()(z)\n",
    "\n",
    "        z = nn.Dense(self.input_features, name='fc_dec_last')(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    latents: int\n",
    "    hidden_dims: Sequence[int]\n",
    "    input_features: int\n",
    "    act: Callable = nn.PReLU\n",
    "    encoder_cls = Encoder\n",
    "    decoder_cls = Decoder\n",
    "    dtype: Any = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, z_rng, train = True):\n",
    "        norm = partial(nn.BatchNorm,\n",
    "                   use_running_average=not train,\n",
    "                   momentum=0.9,\n",
    "                   epsilon=1e-5,\n",
    "                   dtype=self.dtype)\n",
    "\n",
    "        mean, logvar = self.encoder_cls(self.latents,\n",
    "                                    self.hidden_dims,\n",
    "                                    act=self.act,\n",
    "                                    norm=norm)(x)\n",
    "\n",
    "        z = reparameterize(z_rng, mean, logvar)\n",
    "\n",
    "        recon_x = self.decoder_cls(self.hidden_dims,\n",
    "                               self.input_features,\n",
    "                               act=self.act,\n",
    "                               norm=norm)(z)\n",
    "\n",
    "        return recon_x, mean, logvar\n",
    "\n",
    "    def generate(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "  batch_stats: Any\n",
    "\n",
    "def reparameterize(rng, mean, logvar):\n",
    "    std = jnp.exp(0.5 * logvar)\n",
    "    eps = random.normal(rng, logvar.shape)\n",
    "    return mean + eps * std"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from flax import jax_utils\n",
    "\n",
    "def prepare_tf_data(xs):\n",
    "  \"\"\"Convert a input batch from tf Tensors to numpy arrays.\"\"\"\n",
    "  local_device_count = jax.local_device_count()\n",
    "  def _prepare(x):\n",
    "    # Use _numpy() for zero-copy conversion between TF and NumPy.\n",
    "    x = x._numpy()  # pylint: disable=protected-access\n",
    "\n",
    "    # reshape (host_batch_size, height, width, 3) to\n",
    "    # (local_devices, device_batch_size, height, width, 3)\n",
    "    return x.reshape((local_device_count, -1) + x.shape[1:])\n",
    "\n",
    "  return jax.tree_util.tree_map(_prepare, xs)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16, 8) 0 32\n",
      "(5, 5)\n",
      "(32, 16, 8) 1 16\n",
      "(5, 32)\n",
      "(32, 16, 8) 2 8\n",
      "(5, 16)\n",
      "(32, 16, 8) 0 32\n",
      "(5, 5)\n",
      "(32, 16, 8) 1 16\n",
      "(5, 32)\n",
      "(32, 16, 8) 2 8\n",
      "(5, 16)\n"
     ]
    }
   ],
   "source": [
    "test_encoder = Encoder(latents=2, hidden_dims=[32,16,8],\n",
    "                     norm = partial(nn.BatchNorm,use_running_average=True),\n",
    "                     act = nn.PReLU)\n",
    "variables = test_encoder.init(random.PRNGKey(0), jnp.ones((5,5)))\n",
    "y = test_encoder.apply(variables, jnp.ones((5,5)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "(DeviceArray([[-0.56667733,  0.16567218],\n              [-0.56667733,  0.16567218],\n              [-0.56667733,  0.16567218],\n              [-0.56667733,  0.16567218],\n              [-0.56667733,  0.16567218]], dtype=float32),\n DeviceArray([[0.3108176 , 0.15926342],\n              [0.3108176 , 0.15926342],\n              [0.3108176 , 0.15926342],\n              [0.3108176 , 0.15926342],\n              [0.3108176 , 0.15926342]], dtype=float32))"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16, 8) 0 8\n",
      "(5, 2)\n",
      "(32, 16, 8) 1 16\n",
      "(5, 8)\n",
      "(32, 16, 8) 2 32\n",
      "(5, 16)\n",
      "(32, 16, 8) 0 8\n",
      "(5, 2)\n",
      "(32, 16, 8) 1 16\n",
      "(5, 8)\n",
      "(32, 16, 8) 2 32\n",
      "(5, 16)\n"
     ]
    }
   ],
   "source": [
    "test_decoder = Decoder(hidden_dims=[32,16,8],\n",
    "                       input_features=41,\n",
    "                       norm = partial(nn.BatchNorm,use_running_average=True),\n",
    "                       act = nn.PReLU)\n",
    "variables = test_decoder.init(random.PRNGKey(0), jnp.ones((5,2)))\n",
    "y = test_decoder.apply(variables, jnp.ones((5,2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "(5, 41)"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32 16  8] 0 32\n",
      "(5, 41)\n",
      "[32 16  8] 1 16\n",
      "(5, 32)\n",
      "[32 16  8] 2 8\n",
      "(5, 16)\n",
      "[32 16  8] 0 8\n",
      "(5, 2)\n",
      "[32 16  8] 1 16\n",
      "(5, 8)\n",
      "[32 16  8] 2 32\n",
      "(5, 16)\n"
     ]
    }
   ],
   "source": [
    "test_vae = model()\n",
    "variables = test_vae.init(random.PRNGKey(0), jnp.ones((5,41)),random.PRNGKey(1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32 16  8] 0 32\n",
      "(5, 41)\n",
      "[32 16  8] 1 16\n",
      "(5, 32)\n",
      "[32 16  8] 2 8\n",
      "(5, 16)\n",
      "[32 16  8] 0 8\n",
      "(5, 2)\n",
      "[32 16  8] 1 16\n",
      "(5, 8)\n",
      "[32 16  8] 2 32\n",
      "(5, 16)\n"
     ]
    }
   ],
   "source": [
    "y = test_vae.apply(variables, jnp.ones((5,41)),random.PRNGKey(2),train=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "data": {
      "text/plain": "[(5, 41), (5, 2), (5, 2)]"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in y]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "@jax.vmap\n",
    "def kl_divergence(mean, logvar):\n",
    "    return -0.5 * jnp.sum(1 + logvar - jnp.square(mean) - jnp.exp(logvar))\n",
    "\n",
    "@jax.vmap\n",
    "def mse(input, reconstructed):\n",
    "    return jnp.sum((input - reconstructed) ** 2)\n",
    "\n",
    "\n",
    "def compute_metrics(recon_x, x, mean, logvar):\n",
    "    mse_loss = mse(recon_x, x).mean()\n",
    "    kld_loss = kl_divergence(mean, logvar).mean()\n",
    "    return {\n",
    "        'mse': mse_loss,\n",
    "        'kld': kld_loss,\n",
    "        'loss': mse_loss + kld_loss * 0.0025\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch, z_rng):\n",
    "    def loss_fn(params):\n",
    "        print(\"bshape\",batch.shape)\n",
    "        (recon_x, mean, logvar) , _ = model().apply({'params': params, 'batch_stats': state.batch_stats},\n",
    "                                              batch, z_rng, mutable=['batch_stats'])\n",
    "        print(recon_x.shape, mean.shape, logvar.shape)\n",
    "        mse_loss = mse(recon_x, batch).mean()\n",
    "        kld_loss = kl_divergence(mean, logvar).mean()\n",
    "        loss = mse_loss + kld_loss * 0.0025\n",
    "        return loss\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval(params, x, z, z_rng):\n",
    "    def eval_model(vae):\n",
    "        x_recon, mean, logvar = vae(x, z_rng, train = False)\n",
    "\n",
    "        metrics = compute_metrics(x_recon, x, mean, logvar)\n",
    "        return metrics, z\n",
    "\n",
    "    return nn.apply(eval_model, model())({'params': params, 'batch_stats': state.batch_stats})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(0)\n",
    "rng, key = random.split(rng)\n",
    "\n",
    "train_ds = tfds.load('organoids', split='train')\n",
    "train_ds = train_ds.batch(10**7)\n",
    "train_ds = train_ds.cache()\n",
    "train_ds = train_ds.repeat()\n",
    "train_ds = iter(tfds.as_numpy(train_ds))\n",
    "\n",
    "test_ds = tfds.load('organoids', split='validation')\n",
    "test_ds = test_ds.batch(10**6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "(937980, 41)"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_ds)['expression'].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def model():\n",
    "    return VAE(latents=2,\n",
    "               hidden_dims=np.array([32,16,8]),\n",
    "               input_features=41)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bshape (937980, 41)\n",
      "(937980, 41) (937980, 2) (937980, 2)\n",
      "eval epoch: 1, loss: 246.7663, MSE: 246.7651, KLD: 0.4669\n",
      "eval epoch: 2, loss: 244.3150, MSE: 244.3130, KLD: 0.7980\n",
      "eval epoch: 3, loss: 241.8796, MSE: 241.8768, KLD: 1.1182\n",
      "eval epoch: 4, loss: 239.4857, MSE: 239.4822, KLD: 1.3969\n",
      "eval epoch: 5, loss: 236.7507, MSE: 236.7468, KLD: 1.5665\n",
      "eval epoch: 6, loss: 233.6154, MSE: 233.6112, KLD: 1.6602\n",
      "eval epoch: 7, loss: 230.2632, MSE: 230.2588, KLD: 1.7432\n",
      "eval epoch: 8, loss: 226.7554, MSE: 226.7508, KLD: 1.8462\n",
      "eval epoch: 9, loss: 223.0200, MSE: 223.0150, KLD: 1.9846\n",
      "eval epoch: 10, loss: 218.8901, MSE: 218.8847, KLD: 2.1591\n",
      "eval epoch: 11, loss: 214.2507, MSE: 214.2448, KLD: 2.3614\n",
      "eval epoch: 12, loss: 209.0384, MSE: 209.0319, KLD: 2.5764\n",
      "eval epoch: 13, loss: 203.1673, MSE: 203.1603, KLD: 2.7790\n",
      "eval epoch: 14, loss: 196.4226, MSE: 196.4152, KLD: 2.9715\n",
      "eval epoch: 15, loss: 188.6123, MSE: 188.6043, KLD: 3.1871\n",
      "eval epoch: 16, loss: 179.8338, MSE: 179.8250, KLD: 3.4855\n",
      "eval epoch: 17, loss: 170.7032, MSE: 170.6933, KLD: 3.9547\n",
      "eval epoch: 18, loss: 161.3308, MSE: 161.3192, KLD: 4.6310\n",
      "eval epoch: 19, loss: 151.3648, MSE: 151.3512, KLD: 5.4385\n",
      "eval epoch: 20, loss: 140.4028, MSE: 140.3872, KLD: 6.2175\n",
      "eval epoch: 21, loss: 128.5079, MSE: 128.4907, KLD: 6.8563\n",
      "eval epoch: 22, loss: 116.4338, MSE: 116.4154, KLD: 7.3582\n",
      "eval epoch: 23, loss: 105.1791, MSE: 105.1594, KLD: 7.8738\n",
      "eval epoch: 24, loss: 94.9465, MSE: 94.9249, KLD: 8.6334\n",
      "eval epoch: 25, loss: 85.7124, MSE: 85.6878, KLD: 9.8320\n",
      "eval epoch: 26, loss: 77.8596, MSE: 77.8307, KLD: 11.5659\n",
      "eval epoch: 27, loss: 71.4929, MSE: 71.4582, KLD: 13.8711\n",
      "eval epoch: 28, loss: 66.5871, MSE: 66.5452, KLD: 16.7367\n",
      "eval epoch: 29, loss: 63.5469, MSE: 63.4965, KLD: 20.1750\n",
      "eval epoch: 30, loss: 62.7665, MSE: 62.7064, KLD: 24.0637\n",
      "eval epoch: 31, loss: 64.0352, MSE: 63.9651, KLD: 28.0509\n",
      "eval epoch: 32, loss: 66.5096, MSE: 66.4306, KLD: 31.5851\n",
      "eval epoch: 33, loss: 69.2179, MSE: 69.1322, KLD: 34.2666\n",
      "eval epoch: 34, loss: 71.8312, MSE: 71.7412, KLD: 36.0007\n",
      "eval epoch: 35, loss: 74.6198, MSE: 74.5273, KLD: 37.0089\n",
      "eval epoch: 36, loss: 77.9236, MSE: 77.8296, KLD: 37.6308\n",
      "eval epoch: 37, loss: 81.7042, MSE: 81.6086, KLD: 38.2139\n",
      "eval epoch: 38, loss: 85.5543, MSE: 85.4569, KLD: 38.9568\n",
      "eval epoch: 39, loss: 88.8513, MSE: 88.7516, KLD: 39.8454\n",
      "eval epoch: 40, loss: 92.2341, MSE: 92.1315, KLD: 41.0462\n",
      "eval epoch: 41, loss: 97.2583, MSE: 97.1512, KLD: 42.8535\n",
      "eval epoch: 42, loss: 105.4975, MSE: 105.3838, KLD: 45.4730\n",
      "eval epoch: 43, loss: 117.3754, MSE: 117.2534, KLD: 48.7794\n",
      "eval epoch: 44, loss: 131.1977, MSE: 131.0672, KLD: 52.2080\n",
      "eval epoch: 45, loss: 144.5290, MSE: 144.3909, KLD: 55.2409\n",
      "eval epoch: 46, loss: 156.0203, MSE: 155.8764, KLD: 57.5586\n",
      "eval epoch: 47, loss: 166.3517, MSE: 166.2037, KLD: 59.2093\n",
      "eval epoch: 48, loss: 176.7545, MSE: 176.6035, KLD: 60.4047\n",
      "eval epoch: 49, loss: 187.3795, MSE: 187.2263, KLD: 61.2526\n",
      "eval epoch: 50, loss: 197.0820, MSE: 196.9275, KLD: 61.7669\n",
      "eval epoch: 51, loss: 205.4929, MSE: 205.3378, KLD: 62.0334\n",
      "eval epoch: 52, loss: 213.5255, MSE: 213.3699, KLD: 62.2399\n",
      "eval epoch: 53, loss: 224.2613, MSE: 224.1045, KLD: 62.7304\n",
      "eval epoch: 54, loss: 239.0087, MSE: 238.8494, KLD: 63.7016\n",
      "eval epoch: 55, loss: 256.3483, MSE: 256.1855, KLD: 65.1324\n",
      "eval epoch: 56, loss: 275.1332, MSE: 274.9656, KLD: 67.0113\n",
      "eval epoch: 57, loss: 294.7999, MSE: 294.6269, KLD: 69.2104\n",
      "eval epoch: 58, loss: 316.2704, MSE: 316.0912, KLD: 71.6569\n",
      "eval epoch: 59, loss: 338.9542, MSE: 338.7689, KLD: 74.1205\n",
      "eval epoch: 60, loss: 360.8445, MSE: 360.6536, KLD: 76.3380\n",
      "eval epoch: 61, loss: 380.4576, MSE: 380.2620, KLD: 78.2175\n",
      "eval epoch: 62, loss: 400.2467, MSE: 400.0466, KLD: 80.0625\n",
      "eval epoch: 63, loss: 421.9887, MSE: 421.7836, KLD: 82.0674\n",
      "eval epoch: 64, loss: 446.3914, MSE: 446.1807, KLD: 84.2594\n",
      "eval epoch: 65, loss: 468.6733, MSE: 468.4579, KLD: 86.1772\n",
      "eval epoch: 66, loss: 482.9181, MSE: 482.7000, KLD: 87.2364\n",
      "eval epoch: 67, loss: 490.3269, MSE: 490.1080, KLD: 87.5483\n",
      "eval epoch: 68, loss: 494.4754, MSE: 494.2567, KLD: 87.4809\n",
      "eval epoch: 69, loss: 499.5731, MSE: 499.3543, KLD: 87.5168\n",
      "eval epoch: 70, loss: 504.8467, MSE: 504.6275, KLD: 87.7000\n",
      "eval epoch: 71, loss: 509.0150, MSE: 508.7950, KLD: 87.9901\n",
      "eval epoch: 72, loss: 514.5797, MSE: 514.3582, KLD: 88.6106\n",
      "eval epoch: 73, loss: 525.8887, MSE: 525.6639, KLD: 89.9499\n",
      "eval epoch: 74, loss: 544.4764, MSE: 544.2461, KLD: 92.1189\n",
      "eval epoch: 75, loss: 566.8141, MSE: 566.5773, KLD: 94.7363\n",
      "eval epoch: 76, loss: 588.7204, MSE: 588.4770, KLD: 97.3691\n",
      "eval epoch: 77, loss: 608.6284, MSE: 608.3790, KLD: 99.7837\n",
      "eval epoch: 78, loss: 627.3452, MSE: 627.0904, KLD: 101.9346\n",
      "eval epoch: 79, loss: 645.0255, MSE: 644.7659, KLD: 103.8341\n",
      "eval epoch: 80, loss: 659.6241, MSE: 659.3609, KLD: 105.2935\n",
      "eval epoch: 81, loss: 669.4891, MSE: 669.2238, KLD: 106.1272\n",
      "eval epoch: 82, loss: 675.2589, MSE: 674.9930, KLD: 106.3768\n",
      "eval epoch: 83, loss: 681.9512, MSE: 681.6849, KLD: 106.5229\n",
      "eval epoch: 84, loss: 690.4487, MSE: 690.1815, KLD: 106.8864\n",
      "eval epoch: 85, loss: 698.4372, MSE: 698.1686, KLD: 107.4106\n",
      "eval epoch: 86, loss: 706.8868, MSE: 706.6166, KLD: 108.0593\n",
      "eval epoch: 87, loss: 716.0714, MSE: 715.7996, KLD: 108.6987\n",
      "eval epoch: 88, loss: 725.8631, MSE: 725.5897, KLD: 109.3424\n",
      "eval epoch: 89, loss: 736.5711, MSE: 736.2958, KLD: 110.1275\n",
      "eval epoch: 90, loss: 746.5538, MSE: 746.2768, KLD: 110.8063\n",
      "eval epoch: 91, loss: 752.8498, MSE: 752.5723, KLD: 110.9826\n",
      "eval epoch: 92, loss: 756.1101, MSE: 755.8335, KLD: 110.6343\n",
      "eval epoch: 93, loss: 755.9060, MSE: 755.6312, KLD: 109.9257\n",
      "eval epoch: 94, loss: 754.5679, MSE: 754.2950, KLD: 109.1312\n",
      "eval epoch: 95, loss: 753.0082, MSE: 752.7377, KLD: 108.2016\n",
      "eval epoch: 96, loss: 750.9008, MSE: 750.6332, KLD: 107.0613\n",
      "eval epoch: 97, loss: 747.8398, MSE: 747.5753, KLD: 105.8086\n",
      "eval epoch: 98, loss: 745.8438, MSE: 745.5822, KLD: 104.6656\n",
      "eval epoch: 99, loss: 744.6716, MSE: 744.4126, KLD: 103.6022\n",
      "eval epoch: 100, loss: 742.7459, MSE: 742.4897, KLD: 102.4720\n"
     ]
    }
   ],
   "source": [
    "def initialized(key, model):\n",
    "  input_shape = (1, 41)\n",
    "  @jax.jit\n",
    "  def init(*args):\n",
    "    return model.init(*args)\n",
    "  variables = init({'params': key}, jnp.ones(input_shape, model.dtype), key)\n",
    "  return variables['params'], variables['batch_stats']\n",
    "\n",
    "import optax\n",
    "\n",
    "init_data = jnp.ones((937980, 41), jnp.float32)\n",
    "\n",
    "params, batch_stats = initialized(rng,model())\n",
    "state = TrainState.create(\n",
    "      apply_fn=model().apply,\n",
    "      params=params,\n",
    "      tx=optax.adam(0.01),\n",
    "      batch_stats=batch_stats,\n",
    ")\n",
    "\n",
    "rng, z_key, eval_rng = random.split(rng, 3)\n",
    "z = random.normal(z_key, (64, 2))\n",
    "steps_per_epoch = 1\n",
    "\n",
    "for epoch in range(100):\n",
    "    for _ in range(steps_per_epoch):\n",
    "        batch = next(train_ds)['expression']\n",
    "        rng, key = random.split(rng)\n",
    "        state = train_step(state, batch, key)\n",
    "\n",
    "        metrics, _ = eval(state.params, next(iter(test_ds))['expression']._numpy(), z, eval_rng)\n",
    "\n",
    "        print('eval epoch: {}, loss: {:.4f}, MSE: {:.4f}, KLD: {:.4f}'.format(\n",
    "            epoch + 1, metrics['loss'], metrics['mse'], metrics['kld']\n",
    "        ))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "next(iter(test_ds))['expression']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
