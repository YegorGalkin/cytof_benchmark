{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import ray\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from ml_collections import config_dict\n",
    "from ray import tune, air\n",
    "from ray.air import session, Checkpoint\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config = config_dict.ConfigDict()\n",
    "    # General parameters\n",
    "    config.dataset = 'Organoid'\n",
    "    config.model = 'VAE'\n",
    "    config.seed = 12345\n",
    "    config.output_dir = './logs/VanillaVAE/'\n",
    "    config.device = 'cuda'\n",
    "    config.epochs = 10000\n",
    "\n",
    "    # VAE architecture parameters\n",
    "    config.architecture = config_dict.ConfigDict()\n",
    "    config.architecture.in_features = 41\n",
    "    config.architecture.latent_dim = 2\n",
    "    config.architecture.hidden_dims = (32, 32, 32)\n",
    "    config.architecture.kld_weight = 0.0025\n",
    "    config.architecture.loss_type = 'beta'\n",
    "    config.architecture.activation = 'GELU'\n",
    "\n",
    "    # Tunable parameters\n",
    "    config.tunable = config_dict.ConfigDict()\n",
    "    config.tunable.learning_rate = 0.05\n",
    "    config.tunable.weight_decay = 0.0\n",
    "    config.tunable.batch_size = 4096\n",
    "    return config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class BetaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, config: config_dict.ConfigDict) -> None:\n",
    "        super(BetaVAE, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.kld_weight = torch.Tensor([config.architecture.kld_weight]).to(config.device)\n",
    "\n",
    "        self.act_class = getattr(nn, config.architecture.activation)\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        # Build Encoder\n",
    "\n",
    "        encoder_dims = [config.architecture.in_features] + list(config.architecture.hidden_dims)\n",
    "\n",
    "        for i in range(len(config.architecture.hidden_dims)):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_features=encoder_dims[i], out_features=encoder_dims[i + 1]),\n",
    "                    nn.BatchNorm1d(encoder_dims[i + 1]),\n",
    "                    self.act_class(),\n",
    "                )\n",
    "            )\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(config.architecture.hidden_dims[-1], config.architecture.latent_dim)\n",
    "        self.fc_var = nn.Linear(config.architecture.hidden_dims[-1], config.architecture.latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "        decoder_dims = [config.architecture.latent_dim] + list(reversed(config.architecture.hidden_dims)) + [config.architecture.in_features]\n",
    "\n",
    "        for i in range(len(config.architecture.hidden_dims) + 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(decoder_dims[i], decoder_dims[i + 1]),\n",
    "                    nn.BatchNorm1d(decoder_dims[i + 1]),\n",
    "                    self.act_class()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        self.final_layer = nn.Linear(config.architecture.in_features, config.architecture.in_features)\n",
    "\n",
    "    def encode(self, input: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the sample matrix space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C]\n",
    "        \"\"\"\n",
    "        # Only batch - normalized layers\n",
    "        result = self.decoder(z)\n",
    "        # Use linear layer to map normalized decoder outputs back to input space\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> List[torch.Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args) -> dict:\n",
    "        r\"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param epoch: current epoch\n",
    "        :param args:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim=1), dim=0)\n",
    "\n",
    "        loss = recons_loss + self.kld_weight * kld_loss\n",
    "\n",
    "        return {'loss': loss, 'MSE': recons_loss.detach(), 'KLD': -kld_loss.detach()}\n",
    "\n",
    "    def generate(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given an input sample matrix x, returns the reconstructed sample matrix\n",
    "        :param x: (Tensor) [B x C]\n",
    "        :return: (Tensor) [B x C]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "\n",
    "    def latent(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.reparameterize(*self.encode(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from datasets import OrganoidDataset\n",
    "\n",
    "data = OrganoidDataset()\n",
    "\n",
    "X_train, y_train = data.train\n",
    "X_val, y_val = data.val\n",
    "config = get_config()\n",
    "X_train_batches = torch.split(X_train, split_size_or_sections=config.tunable.batch_size)\n",
    "X_val_batches = torch.split(y_val, split_size_or_sections=config.tunable.batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4096, 41])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_batches[0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model = BetaVAE(config).to(config.device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "{'loss': tensor([6.3713], device='cuda:0', grad_fn=<AddBackward0>),\n 'MSE': tensor(6.3707, device='cuda:0'),\n 'KLD': tensor(-0.2079, device='cuda:0')}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss_function(*model.forward(X_train_batches[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def train(model,optimizer,train_dataloader):\n",
    "    for X_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        outputs = model.forward(X_batch)\n",
    "        loss = model.loss_function(*outputs)\n",
    "\n",
    "        loss['loss'].backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def test(model,val_dataloader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        mse,kld,loss = list(),list(),list()\n",
    "        for X_batch in val_dataloader:\n",
    "            loss_dict = dict()\n",
    "            outputs = model.forward(X_batch)\n",
    "            losses = model.loss_function(*outputs)\n",
    "            for key in losses.keys():\n",
    "                loss_dict[key] = losses[key].item() * X_batch.shape[0]\n",
    "            mse.append(loss_dict['MSE'])\n",
    "            kld.append(loss_dict['KLD'])\n",
    "            loss.append(loss_dict['loss'])\n",
    "    data_len = sum(len(batch) for batch in val_dataloader)\n",
    "    return sum(mse)/data_len,sum(kld)/data_len, sum(loss)/data_len\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%        \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "model = BetaVAE(config).to(config.device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),\n",
    "                        lr=config.get(\"learning_rate\", 0.05),\n",
    "                        weight_decay=config.get(\"weight_decay\", 0.0),\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "train(model,optimizer,X_train_batches)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.48099702906890973, -6.172804692370917, 0.49642903957128265)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model,X_val_batches)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def vae_train(cfg):\n",
    "    config = cfg.get('default_config')\n",
    "    model = BetaVAE(config).to(config.device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=config.get(\"learning_rate\", 0.05),\n",
    "                           weight_decay=config.get(\"weight_decay\", 0.0),\n",
    "                           )\n",
    "\n",
    "    dataset = OrganoidDataset(data_dir='/data/PycharmProjects/cytof_benchmark/data/organoids')\n",
    "    X_train, y_train = dataset.train\n",
    "    X_val, y_val = dataset.val\n",
    "    X_train_batches = torch.split(X_train, split_size_or_sections=config.get(\"batch_size\", 16384))\n",
    "    X_val_batches = torch.split(X_val, split_size_or_sections=config.get(\"batch_size\", 16384))\n",
    "\n",
    "    step = 1\n",
    "    if session.get_checkpoint():\n",
    "        checkpoint_dict = session.get_checkpoint().to_dict()\n",
    "\n",
    "        model.load_state_dict(checkpoint_dict[\"model\"])\n",
    "        optimizer.load_state_dict(checkpoint_dict[\"optim\"])\n",
    "        # Note: Make sure to increment the loaded step by 1 to get the\n",
    "        # current step.\n",
    "        last_step = checkpoint_dict[\"step\"]\n",
    "        step = last_step + 1\n",
    "\n",
    "        # NOTE: It's important to set the optimizer learning rates\n",
    "        # again, since we want to explore the parameters passed in by PBT.\n",
    "        # Without this, we would continue using the exact same\n",
    "        # configuration as the trial whose checkpoint we are exploiting.\n",
    "        if \"learning_rate\" in cfg:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = cfg[\"learning_rate\"]\n",
    "        if \"weight_decay\" in cfg:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"weight_decay\"] = cfg[\"weight_decay\"]\n",
    "        print(cfg)\n",
    "    while True:\n",
    "        train(model,optimizer,X_train_batches)\n",
    "        MSE, KLD, loss = test(model,X_val_batches)\n",
    "\n",
    "        checkpoint = None\n",
    "        if step % cfg[\"checkpoint_interval\"] == 0:\n",
    "            checkpoint = Checkpoint.from_dict(\n",
    "                {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optim\": optimizer.state_dict(),\n",
    "                    \"step\": step,\n",
    "                }\n",
    "            )\n",
    "        session.report(\n",
    "            {\n",
    "                \"MSE\": MSE,\n",
    "                \"KLD\": KLD,\n",
    "                \"loss\": loss,\n",
    "                'lr':cfg.get(\"learning_rate\", 0.05),\n",
    "                'wd':cfg.get(\"weight_decay\", 0.0)\n",
    "            },\n",
    "            checkpoint=checkpoint,\n",
    "        )\n",
    "        step += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:03:45,773\tINFO trainable.py:790 -- Restored on 192.168.2.8 from checkpoint: /home/egor/ray_results/vae_training/vae_train_9e3c4_00001_1_2023-01-09_13-03-36/checkpoint_tmpe29624\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:03:45,773\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 9.446563720703125, '_episodes_total': 0}\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:03:55,516\tINFO trainable.py:790 -- Restored on 192.168.2.8 from checkpoint: /home/egor/ray_results/vae_training/vae_train_9e3c4_00000_0_2023-01-09_13-03-24/checkpoint_tmpbe5c5d\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:03:55,516\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 10.27185845375061, '_episodes_total': 0}\n",
      "2023-01-09 13:04:05,110\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9e3c4_00001 (score = -0.588284) into trial 9e3c4_00000 (score = -0.601176)\n",
      "\n",
      "2023-01-09 13:04:05,112\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9e3c4_00000:\n",
      "learning_rate : 1e-05 --- (* 1.2) --> 1.2e-05\n",
      "weight_decay : 1e-09 --- (* 1.2) --> 1.2e-09\n",
      "batch_size : 16384 --- (* 1.2) --> 19660\n",
      "\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:05,218\tINFO trainable.py:790 -- Restored on 192.168.2.8 from checkpoint: /home/egor/ray_results/vae_training/vae_train_9e3c4_00000_0_2023-01-09_13-03-24/checkpoint_tmp6cea92\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:05,218\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 19.025539875030518, '_episodes_total': 0}\n",
      "2023-01-09 13:04:14,690\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9e3c4_00001 (score = -0.588284) into trial 9e3c4_00000 (score = -0.618250)\n",
      "\n",
      "2023-01-09 13:04:14,692\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9e3c4_00000:\n",
      "learning_rate : 1e-05 --- (* 0.8) --> 8.000000000000001e-06\n",
      "weight_decay : 1e-09 --- (* 1.2) --> 1.2e-09\n",
      "batch_size : 16384 --- (* 0.8) --> 13107\n",
      "\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:14,788\tINFO trainable.py:790 -- Restored on 192.168.2.8 from checkpoint: /home/egor/ray_results/vae_training/vae_train_9e3c4_00000_0_2023-01-09_13-03-24/checkpoint_tmpefcd3b\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:14,788\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 19.025539875030518, '_episodes_total': 0}\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:24,480\tINFO trainable.py:790 -- Restored on 192.168.2.8 from checkpoint: /home/egor/ray_results/vae_training/vae_train_9e3c4_00001_1_2023-01-09_13-03-36/checkpoint_tmp6b16ba\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:24,480\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 19.025539875030518, '_episodes_total': 0}\n",
      "2023-01-09 13:04:33,953\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9e3c4_00000 (score = -0.586637) into trial 9e3c4_00001 (score = -0.610239)\n",
      "\n",
      "2023-01-09 13:04:33,954\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9e3c4_00001:\n",
      "learning_rate : 8.000000000000001e-06 --- (resample) --> 0.0011510527735778125\n",
      "weight_decay : 1.2e-09 --- (* 0.8) --> 9.6e-10\n",
      "batch_size : 13107 --- (* 1.2) --> 15728\n",
      "\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:34,058\tINFO trainable.py:790 -- Restored on 192.168.2.8 from checkpoint: /home/egor/ray_results/vae_training/vae_train_9e3c4_00000_0_2023-01-09_13-03-24/checkpoint_tmp51a968\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:34,059\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 28.56000828742981, '_episodes_total': 0}\n",
      "2023-01-09 13:04:43,413\tINFO pbt.py:646 -- [pbt]: no checkpoint for trial. Skip exploit for Trial vae_train_9e3c4_00000\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:43,509\tINFO trainable.py:790 -- Restored on 192.168.2.8 from checkpoint: /home/egor/ray_results/vae_training/vae_train_9e3c4_00001_1_2023-01-09_13-03-36/checkpoint_tmp39716a\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:43,509\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 28.56000828742981, '_episodes_total': 0}\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:53,158\tINFO trainable.py:790 -- Restored on 192.168.2.8 from checkpoint: /home/egor/ray_results/vae_training/vae_train_9e3c4_00000_0_2023-01-09_13-03-24/checkpoint_tmpcf096a\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:04:53,158\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 37.90945315361023, '_episodes_total': 0}\n",
      "2023-01-09 13:05:02,261\tINFO pbt.py:804 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 9e3c4_00001 (score = -0.611012) into trial 9e3c4_00000 (score = -0.636373)\n",
      "\n",
      "2023-01-09 13:05:02,263\tINFO pbt.py:831 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial9e3c4_00000:\n",
      "learning_rate : 0.0011510527735778125 --- (resample) --> 0.000212119981954473\n",
      "weight_decay : 9.6e-10 --- (* 1.2) --> 1.152e-09\n",
      "batch_size : 15728 --- (* 1.2) --> 18873\n",
      "\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:05:02,367\tINFO trainable.py:790 -- Restored on 192.168.2.8 from checkpoint: /home/egor/ray_results/vae_training/vae_train_9e3c4_00000_0_2023-01-09_13-03-24/checkpoint_tmpd4b5c6\n",
      "\u001B[2m\u001B[36m(vae_train pid=28579)\u001B[0m 2023-01-09 13:05:02,367\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 38.06019067764282, '_episodes_total': 0}\n",
      "2023-01-09 13:05:05,367\tWARNING tune.py:690 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [48], line 36\u001B[0m\n\u001B[1;32m     13\u001B[0m smoke_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# For testing purposes: set this to False to run the full experiment\u001B[39;00m\n\u001B[1;32m     14\u001B[0m tuner \u001B[38;5;241m=\u001B[39m tune\u001B[38;5;241m.\u001B[39mTuner(\n\u001B[1;32m     15\u001B[0m     tune\u001B[38;5;241m.\u001B[39mwith_resources(vae_train, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m16\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m2\u001B[39m}),\n\u001B[1;32m     16\u001B[0m     run_config\u001B[38;5;241m=\u001B[39mair\u001B[38;5;241m.\u001B[39mRunConfig(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     },\n\u001B[1;32m     35\u001B[0m )\n\u001B[0;32m---> 36\u001B[0m results_grid \u001B[38;5;241m=\u001B[39m \u001B[43mtuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/tune/tuner.py:272\u001B[0m, in \u001B[0;36mTuner.fit\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_ray_client:\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 272\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_local_tuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    274\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m TuneError(\n\u001B[1;32m    275\u001B[0m             _TUNER_FAILED_MSG\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    276\u001B[0m                 path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_local_tuner\u001B[38;5;241m.\u001B[39mget_experiment_checkpoint_dir()\n\u001B[1;32m    277\u001B[0m             )\n\u001B[1;32m    278\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:420\u001B[0m, in \u001B[0;36mTunerInternal.fit\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_restored:\n\u001B[1;32m    419\u001B[0m     param_space \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param_space)\n\u001B[0;32m--> 420\u001B[0m     analysis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_space\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    422\u001B[0m     analysis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_resume(trainable)\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:532\u001B[0m, in \u001B[0;36mTunerInternal._fit_internal\u001B[0;34m(self, trainable, param_space)\u001B[0m\n\u001B[1;32m    518\u001B[0m \u001B[38;5;124;03m\"\"\"Fitting for a fresh Tuner.\"\"\"\u001B[39;00m\n\u001B[1;32m    519\u001B[0m args \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    520\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_tune_run_arguments(trainable),\n\u001B[1;32m    521\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    530\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tuner_kwargs,\n\u001B[1;32m    531\u001B[0m }\n\u001B[0;32m--> 532\u001B[0m analysis \u001B[38;5;241m=\u001B[39m \u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    533\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    534\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclear_remote_string_queue()\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m analysis\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/tune/tune.py:726\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote, _remote_string_queue)\u001B[0m\n\u001B[1;32m    719\u001B[0m progress_reporter\u001B[38;5;241m.\u001B[39msetup(\n\u001B[1;32m    720\u001B[0m     start_time\u001B[38;5;241m=\u001B[39mtune_start,\n\u001B[1;32m    721\u001B[0m     total_samples\u001B[38;5;241m=\u001B[39msearch_alg\u001B[38;5;241m.\u001B[39mtotal_samples,\n\u001B[1;32m    722\u001B[0m     metric\u001B[38;5;241m=\u001B[39mmetric,\n\u001B[1;32m    723\u001B[0m     mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[1;32m    724\u001B[0m )\n\u001B[1;32m    725\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m runner\u001B[38;5;241m.\u001B[39mis_finished() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msignal\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 726\u001B[0m     \u001B[43mrunner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    727\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m has_verbosity(Verbosity\u001B[38;5;241m.\u001B[39mV1_EXPERIMENT):\n\u001B[1;32m    728\u001B[0m         _report_progress(runner, progress_reporter)\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:981\u001B[0m, in \u001B[0;36mTrialRunner.step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    978\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m next_trial:\n\u001B[1;32m    979\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot new trial to run: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnext_trial\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 981\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_and_handle_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_trial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop_experiment_if_needed()\n\u001B[1;32m    985\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:924\u001B[0m, in \u001B[0;36mTrialRunner._wait_and_handle_event\u001B[0;34m(self, next_trial)\u001B[0m\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wait_and_handle_event\u001B[39m(\u001B[38;5;28mself\u001B[39m, next_trial: Optional[Trial]):\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    923\u001B[0m         \u001B[38;5;66;03m# Single wait of entire tune loop.\u001B[39;00m\n\u001B[0;32m--> 924\u001B[0m         event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrial_executor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_next_executor_event\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_live_trials\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_trial\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    927\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m event\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m _ExecutorEventType\u001B[38;5;241m.\u001B[39mPG_READY:\n\u001B[1;32m    928\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_pg_ready(next_trial)\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py:1025\u001B[0m, in \u001B[0;36mRayTrialExecutor.get_next_executor_event\u001B[0;34m(self, live_trials, next_trial_exists)\u001B[0m\n\u001B[1;32m   1016\u001B[0m     futures_to_wait \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1017\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pg_manager\u001B[38;5;241m.\u001B[39mget_staging_future_list() \u001B[38;5;241m+\u001B[39m futures_to_wait\n\u001B[1;32m   1018\u001B[0m     )\n\u001B[1;32m   1019\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m   1020\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget_next_executor_event before wait with futures \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1021\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfutures_to_wait\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1022\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_trial_exists=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnext_trial_exists\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1023\u001B[0m )\n\u001B[0;32m-> 1025\u001B[0m ready_futures, _ \u001B[38;5;241m=\u001B[39m \u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfutures_to_wait\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_returns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_next_event_wait\u001B[49m\n\u001B[1;32m   1027\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;66;03m###################################################################\u001B[39;00m\n\u001B[1;32m   1030\u001B[0m \u001B[38;5;66;03m# Dealing with no future returned case.\u001B[39;00m\n\u001B[1;32m   1031\u001B[0m \u001B[38;5;66;03m###################################################################\u001B[39;00m\n\u001B[1;32m   1032\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(ready_futures) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001B[0m, in \u001B[0;36mclient_mode_hook.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minit\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m is_client_mode_enabled_by_default:\n\u001B[1;32m    104\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(ray, func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/data/PycharmProjects/cytof_benchmark/venv/lib/python3.10/site-packages/ray/_private/worker.py:2496\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_refs, num_returns, timeout, fetch_local)\u001B[0m\n\u001B[1;32m   2494\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeout \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m10\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m6\u001B[39m\n\u001B[1;32m   2495\u001B[0m timeout_milliseconds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m)\n\u001B[0;32m-> 2496\u001B[0m ready_ids, remaining_ids \u001B[38;5;241m=\u001B[39m \u001B[43mworker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcore_worker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobject_refs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2498\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_returns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2499\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout_milliseconds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2500\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_task_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2501\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfetch_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2502\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2503\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ready_ids, remaining_ids\n",
      "File \u001B[0;32mpython/ray/_raylet.pyx:1759\u001B[0m, in \u001B[0;36mray._raylet.CoreWorker.wait\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpython/ray/_raylet.pyx:195\u001B[0m, in \u001B[0;36mray._raylet.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "perturbation_interval = 5\n",
    "scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"time_total_s\",\n",
    "    perturbation_interval=perturbation_interval,\n",
    "    hyperparam_mutations={\n",
    "        # Distribution for resampling\n",
    "        \"learning_rate\": tune.loguniform(1e-5, 1e-2),\n",
    "        \"weight_decay\": tune.loguniform(1e-10, 1e-6),\n",
    "        \"batch_size\":tune.randint(1024,16*1024)\n",
    "    },\n",
    ")\n",
    "\n",
    "smoke_test = True  # For testing purposes: set this to False to run the full experiment\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(vae_train, {\"cpu\": 16, \"gpu\": 2}),\n",
    "    run_config=air.RunConfig(\n",
    "        name=\"vae_training\",\n",
    "        stop={\"training_iteration\": 5 if smoke_test else 150},\n",
    "        verbose=1,\n",
    "    ),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=2 if smoke_test else 8,\n",
    "        scheduler=scheduler,\n",
    "    ),\n",
    "    param_space={\n",
    "        # Define how initial values of the learning rates should be chosen.\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 1e-9,\n",
    "        \"batch_size\": 16384,\n",
    "        \"checkpoint_interval\": perturbation_interval,\n",
    "        \"default_config\":get_config()\n",
    "    },\n",
    ")\n",
    "results_grid = tuner.fit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
